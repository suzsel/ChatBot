{
 "cells": [
  {
   "cell_type": "code",
   "id": "2bd7b192-a75c-4221-aa1f-b94c60848b1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T23:51:55.460363Z",
     "start_time": "2025-06-03T23:51:55.436324Z"
    }
   },
   "source": "%env OPENAI_API_KEY=",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=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\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "06013f57-637a-44af-ba54-e122cac9fc3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T00:16:34.766190Z",
     "start_time": "2025-06-04T00:16:34.200042Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import docx  \n",
    "import re\n",
    "\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "078baf13-45c5-43d5-9c8c-a479b4bfdcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ TEXT LOADERS ############\n",
    "# Functions to read different file types\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "        # Extract text from each page\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def read_word(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def clean_text(chunks):\n",
    "    cleaned_chunks = []\n",
    "    for chunk in chunks:\n",
    "        # Replace sequences of 2 or more dots with a single space\n",
    "        chunk = re.sub(r\"\\.{2,}\", \" \", chunk, flags=re.UNICODE)\n",
    "        # Replace dots surrounded by spaces with a single space\n",
    "        chunk = re.sub(r\"\\s*\\.\\s*\", \" \", chunk, flags=re.UNICODE)\n",
    "        # Remove non-printable characters (excluding Unicode printable characters, such as Cyrillic)\n",
    "        chunk = re.sub(r\"[^\\x20-\\x7E\\u0400-\\u04FF]+\", \" \", chunk)\n",
    "        # Strip leading/trailing spaces\n",
    "        chunk = chunk.strip()\n",
    "        cleaned_chunks.append(chunk)\n",
    "    return cleaned_chunks\n",
    "\n",
    "    \n",
    "def read_documents_from_directory(directory):\n",
    "    combined_text = \"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            combined_text += read_pdf(file_path)\n",
    "        elif filename.endswith(\".docx\"):\n",
    "            combined_text += read_word(file_path)\n",
    "        elif filename.endswith(\".txt\"):\n",
    "            combined_text += read_txt(file_path)\n",
    "    \n",
    "    return clean_text(combined_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2e3c3d7b-02b3-471f-b173-c3a97036014c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T00:43:31.476872Z",
     "start_time": "2025-06-04T00:43:31.444183Z"
    }
   },
   "source": [
    "train_directory = 'train_data/'\n",
    "# text = read_documents_from_directory(train_directory)\n",
    "text = read_pdf('train_data/filter_doc.pdf')\n",
    "\n",
    "# split into chunks\n",
    "char_text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=1000, \n",
    "                                      chunk_overlap=200, length_function=len)\n",
    "\n",
    "text_chunks = char_text_splitter.split_text(text)\n",
    "cleaned_chunks = clean_text(text_chunks)\n",
    "cleaned_chunks = cleaned_chunks[1:]\n",
    "# len(cleaned_chunks), cleaned_chunks\n",
    "len(cleaned_chunks), cleaned_chunks"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_pdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m train_directory \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_data/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# text = read_documents_from_directory(train_directory)\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[43mread_pdf\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_data/filter_doc.pdf\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# split into chunks\u001B[39;00m\n\u001B[0;32m      6\u001B[0m char_text_splitter \u001B[38;5;241m=\u001B[39m CharacterTextSplitter(separator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, chunk_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, \n\u001B[0;32m      7\u001B[0m                                       chunk_overlap\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m, length_function\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'read_pdf' is not defined"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "eeac8690-c3b8-446f-9d2a-7a3df5700a1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T00:43:27.692930Z",
     "start_time": "2025-06-04T00:43:27.452083Z"
    }
   },
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = FAISS.from_texts(text_chunks, embeddings)\n",
    "\n",
    "\n",
    "llm = OpenAI()\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilyan\\AppData\\Local\\Temp\\ipykernel_39460\\3235116729.py:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m OpenAIEmbeddings()\n\u001B[1;32m----> 2\u001B[0m docsearch \u001B[38;5;241m=\u001B[39m FAISS\u001B[38;5;241m.\u001B[39mfrom_texts(\u001B[43mtext_chunks\u001B[49m, embeddings)\n\u001B[0;32m      5\u001B[0m llm \u001B[38;5;241m=\u001B[39m OpenAI()\n\u001B[0;32m      6\u001B[0m chain \u001B[38;5;241m=\u001B[39m load_qa_chain(llm, chain_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstuff\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'text_chunks' is not defined"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "23da49d0-edb9-4e74-b48b-4b32c6fac8c2",
   "metadata": {},
   "source": [
    "### Saving FAISS-indeces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6bf6cb6b-9a8b-4e87-980c-c4fc2da69bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "index_file = \"Filter/faiss_index.bin\"\n",
    "faiss.write_index(docsearch.index, index_file)\n",
    "\n",
    "# Save metadata if needed (e.g., text chunks)\n",
    "import pickle\n",
    "with open(\"Filter/filter.pkl\", \"wb\") as f:\n",
    "    pickle.dump(text_chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "cf9c85f9-4a04-4528-ac2c-70e69808c176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "я замерил что давление в каплеуловитель 0.05 MPa, нормально ?\n",
      "\n",
      "No, it is not normal. The maximum allowed pressure for the kapeulovitel (drop catcher) is 0.04 MPa, so 0.05 MPa is slightly above the maximum allowed pressure. It is important to make sure that the pressure stays within the specified range to ensure efficient and reliable operation of the scrubber. \n",
      "Tokens Used: 2024\n",
      "\tPrompt Tokens: 1967\n",
      "\tCompletion Tokens: 57\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0030645\n"
     ]
    }
   ],
   "source": [
    "query = \"я замерил что давление в каплеуловитель 0.05 MPa, нормально ?\"\n",
    "docs = docsearch.similarity_search(query, k=4)\n",
    " \n",
    "response = chain.run(input_documents=docs, question=query)\n",
    "print(\" \")\n",
    "print(query)\n",
    "print(response)\n",
    "  \n",
    "#If you want to keep track of your spending\n",
    "with get_openai_callback() as cb:\n",
    "    response = chain.run(input_documents=docs, question=prompt) \n",
    "    print(cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d3ff45ef-4ddb-40c6-8bb8-92628807c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = list(map(int, query.encode('utf-8')))\n",
    "def get_stats(arr):\n",
    "    counts = {}\n",
    "    # pairs = [f\"{x}{y}\" for x, y in zip(arr, arr[1:])]\n",
    "    for pair in zip(arr, arr[1:]):\n",
    "        counts[pair] = counts.get(pair,0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(arr)\n",
    "max_key = max(stats, key=stats.get)\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "d7b1e96c-b56b-4d1e-8c0b-370633189424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges = {\"hi\" : 1, \"by\" : 2}\n",
    "merges.get(\"di\", float('inf'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
